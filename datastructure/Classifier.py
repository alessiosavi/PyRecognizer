# -*- coding: utf-8 -*-
"""
Core utils for manage face recognition process
"""
import json
import logging
import os
import pickle
import time
from pprint import pformat

import face_recognition
from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report, \
    precision_score
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.neural_network import MLPClassifier
from tqdm import tqdm

from datastructure.Person import Person
from utils.util import dump_dataset, load_image_file

log = logging.getLogger()


class Classifier(object):
    """
    Store the knowledge related to the people faces
    """

    def __init__(self):
        self.training_dir = None
        self.model_path = None
        self.peoples_list = []
        self.classifier = None
        self.parameters = {}

    def init_classifier(self):
        """
        Initialize a new classifier after be sure that necessary data are initalized
        """
        if self.classifier is None:
            log.debug("init_classifier | START!")
            if len(self.parameters) > 0:
                log.debug("init_classifier | Initializing a new classifier ... | {0}".format(
                    pformat(self.__dict__)))
                self.classifier = MLPClassifier(**self.parameters)
            else:
                log.error(
                    "init_classifier | Mandatory parameter not provided | Init a new KNN Classifier")
                self.classifier = MLPClassifier()

    def load_classifier_from_file(self, timestamp):
        """
        Initalize the classifier from file.
        The classifier file rappresent the name of the directory related to the classifier that we want to load.

        The tree structure of the the model folder will be something like this

         Structure:
        model/
        ├── <20190520_095119>/  --> Timestamp in which the model was created
        │   ├── model.dat       -->  Dataset generated by encoding the faces and pickelizing them
        │   ├── model.clf       -->  Classifier delegated to recognize a given face
        │   ├── model.json      -->  Hyperparameters related to the current classifier
        ├── <20190519_210950>/
        │   ├── model.dat
        │   ├── model.clf
        │   ├── model.json
        └── ...

        :param timestamp:
        :return:
        """
        log.debug(
            "load_classifier_from_file | Loading classifier from file ... | File: {}".format(timestamp))

        # Load a trained KNN model (if one was passed in)
        err = None
        if self.classifier is None:
            if self.model_path is None or not os.path.isdir(self.model_path):
                raise Exception("Model folder not provided!")
            # Adding the conventional name used for the classifier -> 'model.clf'
            filename = os.path.join(self.model_path, timestamp, "model.clf")
            log.debug(
                "load_classifier_from_file | Loading classifier from file: {}".format(filename))
            if os.path.isfile(filename):
                log.debug(
                    "load_classifier_from_file | File {} exist!".format(filename))
                with open(filename, 'rb') as f:
                    self.classifier = pickle.load(f)
                log.debug("load_classifier_from_file | Classifier loaded!")
            else:
                err = "load_classifier_from_file | FATAL | File {} DOES NOT EXIST ...".format(
                    filename)
        else:
            err = "load_classifier_from_file | FATAL | Path {} DOES NOT EXIST ...".format(
                self.model_path)
        if err is not None:
            log.error(err)
            log.error("load_classifier_from_file | Seems that the model is gone :/ | Loading an empty classifier for "
                      "training purpouse ...")
            self.classifier = None
        return

    def train(self, X, Y, timestamp):
        """
        Train a new model by the given data [X] related to the given target [Y]
        :param X:
        :param Y:
        :param timestamp:
        """
        log.debug("train | START")
        if self.classifier is None:
            self.init_classifier()

        dump_dataset(X, Y, os.path.join(self.model_path, timestamp))

        start_time = time.time()

        X_train, x_test, Y_train, y_test = train_test_split(
            X, Y, test_size=0.25)
        log.debug("train | Training ...")
        self.classifier.fit(X_train, Y_train)
        log.debug("train | Model Trained!")
        log.debug("train | Checking performance ...")
        y_pred = self.classifier.predict(x_test)
        # Static method
        self.verify_performance(y_test, y_pred)

        return self.dump_model(timestamp=timestamp, classifier=self.classifier), time.time() - start_time

    def tuning(self, X, Y, timestamp):
        """
        Tune the hyperparameter of a new model by the given data [X] related to the given target [Y]

        :param X:
        :param Y:
        :param timestamp:
        :return:
        """
        start_time = time.time()
        dump_dataset(X, Y, os.path.join(self.model_path, timestamp))

        X_train, x_test, Y_train, y_test = train_test_split(
            X, Y, test_size=0.25)
        self.classifier = MLPClassifier(max_iter=200)
        # Hyperparameter of the neural network (MLP) to tune
        parameter_space = {
            'hidden_layer_sizes': [(200,),(100,200,100),(100,),],
            #'activation': ['identity', 'tanh', 'relu'],
            'activation': ['identity'],
            'solver': ['adam'],
            'learning_rate': ['constant'],
        }
        log.debug("tuning | Parameter -> {}".format(pformat(parameter_space)))
        grid = GridSearchCV(self.classifier, parameter_space,
                            cv=2, scoring='accuracy', verbose=20, n_jobs=None, pre_dispatch=1)
        grid.fit(X_train, Y_train)
        log.info("TUNING COMPLETE | DUMPING DATA!")
        # log.info("tuning | Grid Scores: {}".format(pformat(grid.grid_scores_)))
        log.info('Best parameters found: {}'.format(grid.best_params_))

        y_pred = grid.predict(x_test)

        log.info('Results on the test set: {}'.format(
            pformat(grid.score(x_test, y_test))))

        self.verify_performance(y_test, y_pred)

        return self.dump_model(timestamp=timestamp, params=grid.best_params_,
                               classifier=grid.best_estimator_), time.time() - start_time

    @staticmethod
    def verify_performance(y_test, y_pred):
        """
        Verify the performance of the result analyzing the known-predict result
        :param y_test:
        :param y_pred:
        :return:
        """

        log.debug("verify_performance | Analyzing performance ...")
        log.info("\nClassification Report: {}".format(
            pformat(classification_report(y_test, y_pred))))
        log.info("balanced_accuracy_score: {}".format(
            pformat(balanced_accuracy_score(y_test, y_pred))))
        log.info("accuracy_score: {}".format(
            pformat(accuracy_score(y_test, y_pred))))
        log.info("precision_score: {}".format(
            pformat(precision_score(y_test, y_pred, average='weighted'))))

    def dump_model(self, timestamp, classifier, params=None, path=None):
        """
        Dump the model to the given path, file
        :param params:
        :param timestamp:
        :param classifier:
        :param path:

        """
        log.debug("dump_model | Dumping model ...")
        if path is None:
            if self.model_path is not None:
                if os.path.exists(self.model_path) and os.path.isdir(self.model_path):
                    path = self.model_path
        config = {'classifier_file': os.path.join(timestamp, "model.clf"),
                  'params': params
                  }
        if not os.path.isdir(path):
            os.makedirs(timestamp)
        classifier_folder = os.path.join(path, timestamp)
        classifier_file = os.path.join(classifier_folder, "model")

        log.debug("dump_model | Dumping model ... | Path: {} | Model folder: {}".format(
            path, timestamp))
        if not os.path.exists(classifier_folder):
            os.makedirs(classifier_folder)

        with open(classifier_file + ".clf", 'wb') as f:
            pickle.dump(classifier, f)
            log.info('dump_model | Model saved to {0}.clf'.format(
                classifier_file))

        with open(classifier_file + ".json", 'w') as f:
            json.dump(config, f)
            log.info('dump_model | Configuration saved to {0}.json'.format(
                classifier_file))

        return config

    def init_peoples_list(self, peoples_path=None):
        """
        This method is delegated to iterate among the folder that contains the peoples's face in order to
        initalize the array of peoples
        :return:
        """

        log.debug("init_peoples_list | Initalizing people ...")
        if peoples_path is not None and os.path.isdir(peoples_path):
            self.training_dir = peoples_path
        else:
            raise Exception("Dataset (peoples faces) path not provided :/")
        # The init process can be threadized, but BATCH method will perform better
        # pool = ThreadPool(3)
        # self.peoples_list = pool.map(self.init_peoples_list_core, os.listdir(self.training_dir))

        for people_name in tqdm(os.listdir(self.training_dir),
                                total=len(os.listdir(self.training_dir)), desc="Init people list ..."):
            self.peoples_list.append(self.init_peoples_list_core(people_name))

        self.peoples_list = list(
            filter(None.__ne__, self.peoples_list))  # Remove None

    def init_peoples_list_core(self, people_name):
        """
        Delegated core method for parallelize operation
        :param people_name:
        :return:
        """
        if os.path.isdir(os.path.join(self.training_dir, people_name)):
            log.debug("Initalizing people {0}".format(
                os.path.join(self.training_dir, people_name)))
            person = Person()
            person.name = people_name
            person.path = os.path.join(self.training_dir, people_name)
            person.init_dataset()
            return person
        else:
            log.debug("People {0} invalid folder!".format(
                os.path.join(self.training_dir, people_name)))
            return None

    def init_dataset(self):
        """
        Initialize a new dataset joining all the data related to the peoples list
        :return:
        """
        DATASET = {
            # Image data (numpy array)
            "X": [],
            # Person name
            "Y": []
        }

        for people in self.peoples_list:
            log.debug(people.name)
            for item in people.dataset["X"]:
                DATASET["X"].append(item)
            for item in people.dataset["Y"]:
                DATASET["Y"].append(item)
        return DATASET

    # The method is delegated to try to retrieve the face from the given image.
    # In case of cuda_malloc error (out of memory), the image will be resized
    @staticmethod
    def extract_face_from_image(X_img_path):
        # Load image data in a numpy array
        try:
            log.debug("predict | Loading image {}".format(X_img_path))
            X_img = load_image_file(X_img_path)
        except OSError:
            log.error("predict | What have you uploaded ???")
            return -2, -2
        log.debug("predict | Extracting faces locations ...")
        try:
            X_face_locations = face_recognition.face_locations(
                X_img, model="cnn")
        except RuntimeError:
            log.error(
                "predict | Have not enough memory, unload data and retry")
            return None, None

        log.debug("predict | Found {} face(s) for the given image".format(
            len(X_face_locations)))

        # If no faces are found in the image, return an empty result.
        if len(X_face_locations) == 0:
            log.warning("predict | Seems that no faces was found :( ")
            return -3, -3

        # Find encodings for faces in the test iamge
        log.debug("predict | Encoding faces ...")
        # num_jitters increase the distortion check
        faces_encodings = face_recognition.face_encodings(
            X_img, known_face_locations=X_face_locations, num_jitters=100)
        log.debug("predict | Face encoded! | Let's ask to the neural network ...")
        return faces_encodings, X_face_locations

    # TODO: Add configuration parameter for choose the distance_threshold
    def predict(self, X_img_path, distance_threshold=0.45):
        """
        Recognizes faces in given image using a trained KNN classifier

        :param X_img_path: path to image to be recognized
        :param distance_threshold: (optional) distance threshold for face classification. the larger it is,
        the more chance of mis-classifying an unknown person as a known one.
        :return: a list of names and face locations for the recognized faces in the image: [(name, bounding box), ...].
                                        For faces of unrecognized persons, the name 'unknown' will be returned.
        """

        if self.classifier is None:
            log.error(
                "predict | Be sure that you have loaded/trained the nerual network model")
            return None

        faces_encodings, X_face_locations = None, None
        # Resize image if necessary for avoid cuda-malloc error (important for low gpu)
        # In case of error, will be returned back an integer.
        # FIXME: manage gpu memory unload in case of None
        while faces_encodings is None or X_face_locations is None:
            faces_encodings, X_face_locations = Classifier.extract_face_from_image(
                X_img_path)
            # In this case return back the error to the caller
            if isinstance(faces_encodings, int):
                return faces_encodings

        # Use the MLP model to find the best matches for the face(s)
        log.debug("predict | Understanding peoples recognized from NN ...")
        closest_distances = self.classifier.predict(faces_encodings)
        log.debug("predict | Persons recognized: [{}]".format(
            closest_distances))

        log.debug("predict | Asking to the neural network for probability ...")
        predictions = self.classifier.predict_proba(faces_encodings)
        pred = []
        for prediction in predictions:
            pred.append(dict([v for v in sorted(zip(self.classifier.classes_, prediction),
                                                key=lambda c: c[1], reverse=True)[:len(closest_distances)]]))
        log.debug("predict | Predict proba -> {}".format(pred))
        face_prediction = []
        for i in range(len(pred)):
            element = list(pred[i].items())[0]
            log.debug("pred in cycle: {}".format(element))
            face_prediction.append(element)
            #log.debug("predict | *****MIN****| {}".format(min(closest_distances[0][i])))
        log.debug("Scores -> {}".format(face_prediction))

        _predictions = []
        scores = []
        if len(face_prediction) > 0:
            for person_score, loc in zip(face_prediction, X_face_locations):
                if person_score[1] < distance_threshold:
                    log.warning("predict | Person {} does not outbounds treshold {}<{}".format(
                        pred, person_score[1], distance_threshold))
                else:
                    log.debug("predict | Pred: {} | Loc: {} | Score: {}".format(
                        person_score[0], loc, person_score[1]))
                    _predictions.append((person_score[0], loc))
                    scores.append(person_score[1])
            log.debug("predict | Prediction: {}".format(_predictions))
            log.debug("predict | Score: {}".format(scores))

        else:
            log.debug("predict | Face not recognized :/")
            return -1

        return {"predictions": _predictions, "scores": scores}
